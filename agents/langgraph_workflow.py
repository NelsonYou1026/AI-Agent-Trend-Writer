import logging
from typing import Dict, List, TypedDict, Annotated
from dataclasses import dataclass
import json
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import re

# LangGraph imports
from langgraph.graph import StateGraph
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI

from config import settings
from config.agents_config import (
    RESEARCHER_PROMPT,
    SCRIPT_WRITER_PROMPT,
    SOCIAL_WRITER_PROMPT,
    WEB_SCRAPER_PROMPT,
)

logging.basicConfig(level=logging.INFO)


# State å®šç¾©
class WorkflowState(TypedDict):
    topic: str
    trend_urls: List[str]
    website_analyses: List[Dict]
    scraping_codes: List[Dict]
    scraped_data: List[Dict]
    summary: str
    video_script: str
    social_media: str
    progress_callback: callable
    error_messages: List[str]


@dataclass
class WebsiteStructure:
    """ç¶²ç«™çµæ§‹åˆ†æçµæœ"""
    url: str
    title: str
    main_content_selectors: List[str]
    text_selectors: List[str]
    image_selectors: List[str]
    link_selectors: List[str]
    meta_info: Dict
    suggested_approach: str


def get_llm():
    """ç²å–é…ç½®å¥½çš„ LLM"""
    return ChatOpenAI(
        model=settings.OPENAI_MODEL_NAME,
        api_key=settings.OPENAI_API_KEY,
        base_url=settings.OPENAI_API_BASE,
        extra_body={
            "chat_template_kwargs": {"enable_thinking": False},
        },
    )


def analyze_website_structure(url: str) -> WebsiteStructure:
    """
    åˆ†æç¶²ç«™HTMLçµæ§‹ï¼Œç‚ºçˆ¬èŸ²ä»£ç¢¼ç”Ÿæˆæä¾›ä¿¡æ¯
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # åˆ†æç¶²ç«™çµæ§‹
        title = soup.find('title')
        title_text = title.get_text().strip() if title else "No title found"
        
        # å°‹æ‰¾ä¸»è¦å…§å®¹é¸æ“‡å™¨
        main_content_selectors = []
        for selector in ['main', 'article', '.content', '.main-content', '#content', '.post', '.entry']:
            if soup.select(selector):
                main_content_selectors.append(selector)
        
        # å°‹æ‰¾æ–‡æœ¬é¸æ“‡å™¨
        text_selectors = []
        for selector in ['p', 'h1', 'h2', 'h3', '.text', '.description', '.summary']:
            if soup.select(selector):
                text_selectors.append(selector)
        
        # å°‹æ‰¾åœ–ç‰‡é¸æ“‡å™¨
        image_selectors = []
        for selector in ['img', '.image', '.photo', 'figure img']:
            if soup.select(selector):
                image_selectors.append(selector)
        
        # å°‹æ‰¾éˆæ¥é¸æ“‡å™¨
        link_selectors = []
        for selector in ['a[href]', '.link', '.more-link']:
            if soup.select(selector):
                link_selectors.append(selector)
        
        # Meta ä¿¡æ¯
        meta_info = {
            'domain': urlparse(url).netloc,
            'has_json_ld': bool(soup.find_all('script', type='application/ld+json')),
            'has_meta_description': bool(soup.find('meta', {'name': 'description'})),
            'page_lang': soup.find('html', {'lang': True}),
            'total_elements': len(soup.find_all()),
            'has_structured_data': bool(soup.select('[itemscope], [vocab]'))
        }
        
        # å»ºè­°çš„çˆ¬èŸ²æ–¹æ³•
        if meta_info['has_json_ld']:
            suggested_approach = "json-ld"
        elif main_content_selectors:
            suggested_approach = "content-selector"
        elif text_selectors:
            suggested_approach = "text-extraction"
        else:
            suggested_approach = "general-scraping"
        
        return WebsiteStructure(
            url=url,
            title=title_text,
            main_content_selectors=main_content_selectors[:3],  # å–å‰3å€‹
            text_selectors=text_selectors[:5],  # å–å‰5å€‹
            image_selectors=image_selectors[:3],  # å–å‰3å€‹
            link_selectors=link_selectors[:3],  # å–å‰3å€‹
            meta_info=meta_info,
            suggested_approach=suggested_approach
        )
        
    except Exception as e:
        logging.error(f"Error analyzing website structure for {url}: {e}")
        return WebsiteStructure(
            url=url,
            title="Analysis failed",
            main_content_selectors=[],
            text_selectors=['p', 'h1', 'h2', 'h3'],  # é è¨­é¸æ“‡å™¨
            image_selectors=['img'],
            link_selectors=['a'],
            meta_info={'error': str(e)},
            suggested_approach="general-scraping"
        )


# å·¥ä½œæµç¨‹ç¯€é»å‡½æ•¸
def analyze_urls_node(state: WorkflowState) -> WorkflowState:
    """ç¯€é»1: åˆ†æè¶¨å‹¢URLçš„ç¶²ç«™çµæ§‹"""
    logging.info("ğŸ” é–‹å§‹åˆ†æç¶²ç«™çµæ§‹...")
    
    if state.get('progress_callback'):
        state['progress_callback']("ğŸ” æ­£åœ¨åˆ†æè¶¨å‹¢ç¶²ç«™çµæ§‹...")
    
    website_analyses = []
    for url in state['trend_urls'][:3]:  # é™åˆ¶è™•ç†å‰3å€‹URL
        try:
            analysis = analyze_website_structure(url)
            website_analyses.append({
                'url': url,
                'structure': analysis,
                'analysis_success': True
            })
            logging.info(f"âœ… æˆåŠŸåˆ†æ: {url}")
        except Exception as e:
            website_analyses.append({
                'url': url,
                'structure': None,
                'analysis_success': False,
                'error': str(e)
            })
            logging.error(f"âŒ åˆ†æå¤±æ•—: {url} - {e}")
    
    state['website_analyses'] = website_analyses
    return state


def generate_scraping_code_node(state: WorkflowState) -> WorkflowState:
    """ç¯€é»2: Code Agent ç”Ÿæˆçˆ¬èŸ²ç¨‹å¼ç¢¼"""
    logging.info("ğŸ¤– Code Agent ç”Ÿæˆçˆ¬èŸ²ç¨‹å¼ç¢¼...")
    
    if state.get('progress_callback'):
        state['progress_callback']("ğŸ¤– Code Agent æ­£åœ¨ç”Ÿæˆçˆ¬èŸ²ç¨‹å¼ç¢¼...")
    
    llm = get_llm()
    scraping_codes = []
    
    code_agent_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„çˆ¬èŸ²ç¨‹å¼ç¢¼ç”Ÿæˆå°ˆå®¶ã€‚æ ¹æ“šæä¾›çš„ç¶²ç«™çµæ§‹åˆ†æï¼Œç”Ÿæˆé«˜æ•ˆçš„Pythonçˆ¬èŸ²ç¨‹å¼ç¢¼ã€‚

è¦æ±‚ï¼š
1. ç”Ÿæˆçš„ç¨‹å¼ç¢¼å¿…é ˆæ˜¯å®Œæ•´å¯åŸ·è¡Œçš„
2. åŒ…å«éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶
3. æå–ç¶²ç«™çš„ä¸»è¦å…§å®¹ã€æ¨™é¡Œã€æ–‡æœ¬ã€åœ–ç‰‡URLç­‰
4. è¿”å›çµæ§‹åŒ–çš„JSONæ ¼å¼æ•¸æ“š
5. ä½¿ç”¨requestså’ŒBeautifulSoupåº«

ç¶²ç«™çµæ§‹åˆ†æçµæœï¼š
{analysis}

è«‹ç”Ÿæˆé‡å°æ­¤ç¶²ç«™çš„çˆ¬èŸ²ç¨‹å¼ç¢¼ï¼Œç¨‹å¼ç¢¼æ‡‰è©²åŒ…å«ä¸€å€‹mainå‡½æ•¸ï¼Œæ¥å—urlåƒæ•¸ä¸¦è¿”å›çˆ¬å–çµæœã€‚"""

    for website_analysis in state['website_analyses']:
        if not website_analysis['analysis_success']:
            scraping_codes.append({
                'url': website_analysis['url'],
                'code': None,
                'generation_success': False,
                'error': website_analysis.get('error', 'Unknown error')
            })
            continue
            
        try:
            structure = website_analysis['structure']
            analysis_text = f"""
URL: {structure.url}
æ¨™é¡Œ: {structure.title}
å»ºè­°æ–¹æ³•: {structure.suggested_approach}
ä¸»è¦å…§å®¹é¸æ“‡å™¨: {structure.main_content_selectors}
æ–‡æœ¬é¸æ“‡å™¨: {structure.text_selectors}
åœ–ç‰‡é¸æ“‡å™¨: {structure.image_selectors}
Metaä¿¡æ¯: {structure.meta_info}
            """
            
            messages = [
                SystemMessage(content=code_agent_prompt.format(analysis=analysis_text)),
                HumanMessage(content=f"ç‚º {structure.url} ç”Ÿæˆçˆ¬èŸ²ç¨‹å¼ç¢¼")
            ]
            
            response = llm.invoke(messages)
            generated_code = response.content
            
            # å¾å›æ‡‰ä¸­æå–ç¨‹å¼ç¢¼
            if "```python" in generated_code:
                code_start = generated_code.find("```python") + 9
                code_end = generated_code.find("```", code_start)
                if code_end != -1:
                    generated_code = generated_code[code_start:code_end].strip()
            
            scraping_codes.append({
                'url': structure.url,
                'code': generated_code,
                'generation_success': True,
                'structure_info': analysis_text
            })
            
            logging.info(f"âœ… æˆåŠŸç”Ÿæˆçˆ¬èŸ²ç¨‹å¼ç¢¼: {structure.url}")
            
        except Exception as e:
            scraping_codes.append({
                'url': website_analysis['url'],
                'code': None,
                'generation_success': False,
                'error': str(e)
            })
            logging.error(f"âŒ ç¨‹å¼ç¢¼ç”Ÿæˆå¤±æ•—: {website_analysis['url']} - {e}")
    
    state['scraping_codes'] = scraping_codes
    return state


def execute_scraping_code_node(state: WorkflowState) -> WorkflowState:
    """ç¯€é»3: Code Executor Proxy åŸ·è¡Œçˆ¬èŸ²ç¨‹å¼ç¢¼"""
    logging.info("âš™ï¸ åŸ·è¡Œçˆ¬èŸ²ç¨‹å¼ç¢¼...")
    
    if state.get('progress_callback'):
        state['progress_callback']("âš™ï¸ Code Executor æ­£åœ¨åŸ·è¡Œçˆ¬èŸ²ç¨‹å¼ç¢¼...")
    
    scraped_data = []
    
    for code_info in state['scraping_codes']:
        if not code_info['generation_success'] or not code_info['code']:
            scraped_data.append({
                'url': code_info['url'],
                'data': None,
                'execution_success': False,
                'error': code_info.get('error', 'No code to execute')
            })
            continue
        
        try:
            # æº–å‚™åŸ·è¡Œç’°å¢ƒ
            exec_globals = {
                'requests': requests,
                'BeautifulSoup': BeautifulSoup,
                'json': json,
                'urlparse': urlparse,
                'urljoin': urljoin,
                're': re
            }
            
            # åŸ·è¡Œç¨‹å¼ç¢¼
            exec(code_info['code'], exec_globals)
            
            # èª¿ç”¨mainå‡½æ•¸ï¼ˆå‡è¨­ç”Ÿæˆçš„ç¨‹å¼ç¢¼åŒ…å«mainå‡½æ•¸ï¼‰
            if 'main' in exec_globals:
                result = exec_globals['main'](code_info['url'])
                scraped_data.append({
                    'url': code_info['url'],
                    'data': result,
                    'execution_success': True
                })
                logging.info(f"âœ… æˆåŠŸåŸ·è¡Œçˆ¬èŸ²: {code_info['url']}")
            else:
                scraped_data.append({
                    'url': code_info['url'],
                    'data': None,
                    'execution_success': False,
                    'error': 'No main function found in generated code'
                })
                
        except Exception as e:
            scraped_data.append({
                'url': code_info['url'],
                'data': None,
                'execution_success': False,
                'error': str(e)
            })
            logging.error(f"âŒ çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {code_info['url']} - {e}")
    
    state['scraped_data'] = scraped_data
    return state


def summary_agent_node(state: WorkflowState) -> WorkflowState:
    """ç¯€é»4: Summary Agent æ•´åˆçˆ¬èŸ²çµæœ"""
    logging.info("ğŸ“‹ Summary Agent æ•´åˆçˆ¬èŸ²çµæœ...")
    
    if state.get('progress_callback'):
        state['progress_callback']("ğŸ“‹ Summary Agent æ­£åœ¨æ•´åˆçˆ¬èŸ²çµæœ...")
    
    llm = get_llm()
    
    # æ•´ç†æ‰€æœ‰æˆåŠŸçš„çˆ¬èŸ²æ•¸æ“š
    successful_data = []
    for item in state['scraped_data']:
        if item['execution_success'] and item['data']:
            successful_data.append({
                'url': item['url'],
                'content': item['data']
            })
    
    if not successful_data:
        state['summary'] = f"é—œæ–¼ä¸»é¡Œ '{state['topic']}' çš„çˆ¬èŸ²éç¨‹ä¸­æ²’æœ‰ç²å¾—æœ‰æ•ˆæ•¸æ“šï¼Œè«‹æª¢æŸ¥ç¶²ç«™å¯è¨ªå•æ€§æˆ–èª¿æ•´çˆ¬èŸ²ç­–ç•¥ã€‚"
        return state
    
    summary_prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å…§å®¹åˆ†æå°ˆå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹çˆ¬èŸ²ç²å–çš„æ•¸æ“šï¼Œç‚ºä¸»é¡Œ "{state['topic']}" ç”Ÿæˆä¸€å€‹å…¨é¢çš„æ‘˜è¦å ±å‘Šã€‚

çˆ¬èŸ²æ•¸æ“šï¼š
{json.dumps(successful_data, ensure_ascii=False, indent=2)}

è«‹æä¾›ï¼š
1. ä¸»é¡Œçš„æ ¸å¿ƒè¦é»å’Œé—œéµä¿¡æ¯
2. é‡è¦çš„çµ±è¨ˆæ•¸æ“šæˆ–äº‹å¯¦
3. æœ€æ–°çš„ç™¼å±•å‹•æ…‹
4. ç›¸é—œçš„èƒŒæ™¯ä¿¡æ¯
5. å€¼å¾—é—œæ³¨çš„è¶‹åŠ¿æˆ–å½±éŸ¿

è¦æ±‚ï¼š
- å…§å®¹è¦æº–ç¢ºã€å®¢è§€
- é‡é»çªå‡ºï¼Œå±¤æ¬¡åˆ†æ˜
- é©åˆç”¨æ–¼å¾ŒçºŒçš„å½±ç‰‡è…³æœ¬å‰µä½œ
- å­—æ•¸æ§åˆ¶åœ¨800-1200å­—ä¹‹é–“
"""
    
    try:
        messages = [
            SystemMessage(content=summary_prompt),
            HumanMessage(content=f"è«‹ç‚ºä¸»é¡Œ '{state['topic']}' ç”Ÿæˆæ‘˜è¦å ±å‘Š")
        ]
        
        response = llm.invoke(messages)
        state['summary'] = response.content
        logging.info("âœ… æˆåŠŸç”Ÿæˆæ‘˜è¦å ±å‘Š")
        
    except Exception as e:
        state['summary'] = f"æ‘˜è¦ç”Ÿæˆéç¨‹ä¸­å‡ºç¾éŒ¯èª¤ï¼š{str(e)}"
        logging.error(f"âŒ æ‘˜è¦ç”Ÿæˆå¤±æ•—: {e}")
    
    return state


def script_writer_node(state: WorkflowState) -> WorkflowState:
    """ç¯€é»5: Script Writer å‰µä½œå½±ç‰‡è…³æœ¬"""
    logging.info("ğŸ¬ Script Writer å‰µä½œå½±ç‰‡è…³æœ¬...")
    
    if state.get('progress_callback'):
        state['progress_callback']("ğŸ¬ Script Writer æ­£åœ¨å‰µä½œå½±ç‰‡è…³æœ¬...")
    
    llm = get_llm()
    
    script_prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å½±ç‰‡è…³æœ¬ä½œå®¶ã€‚è«‹æ ¹æ“šæä¾›çš„æ‘˜è¦å…§å®¹ï¼Œç‚ºä¸»é¡Œ "{state['topic']}" å‰µä½œä¸€å€‹60ç§’çš„å½±ç‰‡è…³æœ¬ã€‚

æ‘˜è¦å…§å®¹ï¼š
{state['summary']}

è…³æœ¬è¦æ±‚ï¼š
1. æ™‚é•·ï¼šé©åˆ60ç§’å½±ç‰‡ï¼ˆç´„150-180å­—ï¼‰
2. çµæ§‹ï¼šé–‹å ´ç™½(10ç§’) + ä¸»è¦å…§å®¹(40ç§’) + çµè«–(10ç§’)
3. èªèª¿ï¼šå°ˆæ¥­ä½†æ˜“æ‡‚ï¼Œå¸å¼•è§€çœ¾
4. åŒ…å«ï¼šé—œéµçµ±è¨ˆæ•¸æ“šã€å…·é«”äº‹å¯¦ã€è¡Œå‹•å‘¼ç±²
5. æ ¼å¼ï¼šæ¨™æ˜æ™‚é–“ç¯€é»å’Œæ—ç™½å…§å®¹

è«‹å‰µä½œå‡ºå¼•äººå…¥å‹çš„å½±ç‰‡è…³æœ¬ã€‚
"""
    
    try:
        messages = [
            SystemMessage(content=script_prompt),
            HumanMessage(content=f"è«‹ç‚ºä¸»é¡Œ '{state['topic']}' å‰µä½œ60ç§’å½±ç‰‡è…³æœ¬")
        ]
        
        response = llm.invoke(messages)
        state['video_script'] = response.content
        logging.info("âœ… æˆåŠŸå‰µä½œå½±ç‰‡è…³æœ¬")
        
    except Exception as e:
        state['video_script'] = f"å½±ç‰‡è…³æœ¬å‰µä½œéç¨‹ä¸­å‡ºç¾éŒ¯èª¤ï¼š{str(e)}"
        logging.error(f"âŒ å½±ç‰‡è…³æœ¬å‰µä½œå¤±æ•—: {e}")
    
    return state


def social_media_writer_node(state: WorkflowState) -> WorkflowState:
    """ç¯€é»6: Social Media Writer å‰µä½œç¤¾ç¾¤åª’é«”å…§å®¹"""
    logging.info("ğŸ“± Social Media Writer å‰µä½œç¤¾ç¾¤åª’é«”å…§å®¹...")
    
    if state.get('progress_callback'):
        state['progress_callback']("ğŸ“± Social Media Writer æ­£åœ¨å‰µä½œç¤¾ç¾¤åª’é«”å…§å®¹...")
    
    llm = get_llm()
    
    social_prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç¤¾ç¾¤åª’é«”å…§å®¹å‰µä½œè€…ã€‚è«‹æ ¹æ“šå½±ç‰‡è…³æœ¬å’Œæ‘˜è¦å…§å®¹ï¼Œç‚ºä¸»é¡Œ "{state['topic']}" å‰µä½œç¤¾ç¾¤åª’é«”è²¼æ–‡ã€‚

å½±ç‰‡è…³æœ¬ï¼š
{state['video_script']}

æ‘˜è¦å…§å®¹ï¼š
{state['summary']}

è«‹ç‚ºä»¥ä¸‹å¹³å°å‰µä½œå…§å®¹ï¼š

1. **Instagram/Facebook è²¼æ–‡**ï¼š
   - å¸å¼•äººçš„é–‹å ´
   - 2-3å€‹é—œéµé»
   - ç›¸é—œæ¨™ç±¤ (#hashtags)
   - è¡Œå‹•å‘¼ç±²

2. **Twitter/X æ¨æ–‡**ï¼š
   - ç°¡æ½”æœ‰åŠ›ï¼ˆ280å­—ä»¥å…§ï¼‰
   - åŒ…å«1-2å€‹è¦é»
   - ç›¸é—œæ¨™ç±¤

3. **LinkedIn æ–‡ç« **ï¼š
   - å°ˆæ¥­èªèª¿
   - æ·±åº¦åˆ†æ
   - å•†æ¥­åƒ¹å€¼å°å‘
   - å°ˆæ¥­æ¨™ç±¤

æ¯å€‹å¹³å°çš„å…§å®¹è¦ç¬¦åˆå…¶ç‰¹è‰²å’Œç”¨æˆ¶ç¿’æ…£ã€‚
"""
    
    try:
        messages = [
            SystemMessage(content=social_prompt),
            HumanMessage(content=f"è«‹ç‚ºä¸»é¡Œ '{state['topic']}' å‰µä½œå¤šå¹³å°ç¤¾ç¾¤åª’é«”å…§å®¹")
        ]
        
        response = llm.invoke(messages)
        state['social_media'] = response.content
        logging.info("âœ… æˆåŠŸå‰µä½œç¤¾ç¾¤åª’é«”å…§å®¹")
        
    except Exception as e:
        state['social_media'] = f"ç¤¾ç¾¤åª’é«”å…§å®¹å‰µä½œéç¨‹ä¸­å‡ºç¾éŒ¯èª¤ï¼š{str(e)}"
        logging.error(f"âŒ ç¤¾ç¾¤åª’é«”å…§å®¹å‰µä½œå¤±æ•—: {e}")
    
    return state


def create_langgraph_workflow():
    """å‰µå»º LangGraph å·¥ä½œæµç¨‹"""
    
    # å‰µå»ºç‹€æ…‹åœ–
    workflow = StateGraph(WorkflowState)
    
    # æ·»åŠ ç¯€é»
    workflow.add_node("analyze_urls", analyze_urls_node)
    workflow.add_node("generate_code", generate_scraping_code_node)
    workflow.add_node("execute_code", execute_scraping_code_node)
    workflow.add_node("summarize", summary_agent_node)
    workflow.add_node("write_script", script_writer_node)
    workflow.add_node("write_social", social_media_writer_node)
    
    # è¨­å®šæµç¨‹é‚Š
    workflow.set_entry_point("analyze_urls")
    workflow.add_edge("analyze_urls", "generate_code")
    workflow.add_edge("generate_code", "execute_code")
    workflow.add_edge("execute_code", "summarize")
    workflow.add_edge("summarize", "write_script")
    workflow.add_edge("write_script", "write_social")
    workflow.set_finish_point("write_social")
    
    return workflow.compile()


def run_langgraph_workflow(topic: str, trend_urls: List[str], progress_callback=None) -> Dict[str, str]:
    """
    ä½¿ç”¨ LangGraph åŸ·è¡Œå®Œæ•´çš„å·¥ä½œæµç¨‹
    
    Args:
        topic (str): ä¸»é¡Œ
        trend_urls (List[str]): è¶¨å‹¢URLåˆ—è¡¨
        progress_callback (callable): é€²åº¦å›èª¿å‡½æ•¸
    
    Returns:
        Dict[str, str]: åŒ…å«å½±ç‰‡è…³æœ¬å’Œç¤¾ç¾¤åª’é«”å…§å®¹çš„å­—å…¸
    """
    try:
        # å‰µå»ºå·¥ä½œæµç¨‹
        workflow = create_langgraph_workflow()
        
        # åˆå§‹åŒ–ç‹€æ…‹
        initial_state = WorkflowState(
            topic=topic,
            trend_urls=trend_urls,
            website_analyses=[],
            scraping_codes=[],
            scraped_data=[],
            summary="",
            video_script="",
            social_media="",
            progress_callback=progress_callback,
            error_messages=[]
        )
        
        if progress_callback:
            progress_callback("ğŸš€ å•Ÿå‹• LangGraph å·¥ä½œæµç¨‹...")
        
        # åŸ·è¡Œå·¥ä½œæµç¨‹
        final_state = workflow.invoke(initial_state)
        
        if progress_callback:
            progress_callback("âœ… LangGraph å·¥ä½œæµç¨‹åŸ·è¡Œå®Œæˆï¼")
        
        # è¿”å›çµæœ
        return {
            "video_script": final_state.get('video_script', 'å½±ç‰‡è…³æœ¬ç”Ÿæˆå¤±æ•—'),
            "social_media": final_state.get('social_media', 'ç¤¾ç¾¤åª’é«”å…§å®¹ç”Ÿæˆå¤±æ•—'),
            "summary": final_state.get('summary', ''),
            "scraped_data_count": len([d for d in final_state.get('scraped_data', []) if d.get('execution_success')]),
            "processed_urls": [analysis['url'] for analysis in final_state.get('website_analyses', [])]
        }
        
    except Exception as e:
        logging.error(f"âŒ LangGraph å·¥ä½œæµç¨‹åŸ·è¡Œå¤±æ•—: {e}")
        return {
            "video_script": f"å·¥ä½œæµç¨‹åŸ·è¡Œå¤±æ•—ï¼š{str(e)}",
            "social_media": f"å·¥ä½œæµç¨‹åŸ·è¡Œå¤±æ•—ï¼š{str(e)}",
            "summary": "",
            "scraped_data_count": 0,
            "processed_urls": []
        }


if __name__ == '__main__':
    # æ¸¬è©¦ç¯„ä¾‹
    test_topic = "AI æŠ€è¡“ç™¼å±•è¶¨å‹¢"
    test_urls = [
        "https://example.com/ai-news1",
        "https://example.com/ai-news2", 
        "https://example.com/ai-news3"
    ]
    
    def test_callback(message):
        print(f"é€²åº¦æ›´æ–°: {message}")
    
    result = run_langgraph_workflow(test_topic, test_urls, test_callback)
    
    print("=== å½±ç‰‡è…³æœ¬ ===")
    print(result["video_script"])
    print("\n=== ç¤¾ç¾¤åª’é«”å…§å®¹ ===")
    print(result["social_media"])