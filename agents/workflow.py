import logging
from typing import Dict, List

import autogen
from autogen import Agent, AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager

from config import settings
from config.agents_config import (
    RESEARCHER_PROMPT,
    SCRIPT_WRITER_PROMPT,
    SOCIAL_WRITER_PROMPT,
    WEB_SCRAPER_PROMPT,
)

# å”èª¿è€… Agent æç¤ºè©
COORDINATOR_PROMPT = """
ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å·¥ä½œæµç¨‹å”èª¿è€… (Workflow Coordinator)ã€‚ä½ çš„è·è²¬æ˜¯ï¼š

1. **ç›£æ§é€²åº¦**: ç¢ºä¿æ‰€æœ‰ Agent æŒ‰è¨ˆåŠƒå®Œæˆä»»å‹™
2. **è³ªé‡æ§åˆ¶**: æª¢æŸ¥æ¯å€‹ Agent çš„è¼¸å‡ºæ˜¯å¦ç¬¦åˆè¦æ±‚
3. **æµç¨‹ç®¡ç†**: ç¢ºä¿å·¥ä½œæµç¨‹é †åˆ©é€²è¡Œ
4. **æœ€çµ‚ç¢ºèª**: ç•¶æ‰€æœ‰å·¥ä½œå®Œæˆæ™‚ï¼Œå®£å¸ƒå·¥ä½œå®Œæˆä¸¦è§¸ç™¼åœæ­¢

å·¥ä½œæµç¨‹æª¢æŸ¥æ¸…å–®ï¼š
- âœ… Trend_Researcher æ˜¯å¦å®Œæˆä¸»é¡Œç ”ç©¶ä¸¦æä¾›çµæ§‹åŒ–è³‡æ–™ï¼Ÿ
- âœ… Web_Scraper æ˜¯å¦æˆåŠŸçˆ¬å–ç›¸é—œç¶²é ä¸¦æä¾›è©³ç´°è³‡æ–™ï¼Ÿ
- âœ… Script_Writer æ˜¯å¦åŸºæ–¼ç ”ç©¶è³‡æ–™å‰µä½œäº† 60 ç§’å½±ç‰‡è…³æœ¬ï¼Ÿ
- âœ… Social_Media_Writer æ˜¯å¦å‰µä½œäº†ç¤¾ç¾¤åª’é«”æ–‡æ¡ˆï¼Ÿ
- âœ… æ‰€æœ‰å…§å®¹æ˜¯å¦å“è³ªè‰¯å¥½ï¼Œç¬¦åˆè¦æ±‚ï¼Ÿ

ç•¶æ‰€æœ‰ä»»å‹™éƒ½å®Œæˆä¸”å“è³ªåˆæ ¼æ™‚ï¼Œä½ å¿…é ˆå›è¦†ï¼š
"WORKFLOW_COMPLETE - æ‰€æœ‰ä»»å‹™å·²å®Œæˆï¼Œå“è³ªæª¢æŸ¥é€šéï¼

è«‹ user_proxy æŒ‰ç…§ä»¥ä¸‹ç²¾ç¢ºæ ¼å¼æ•´ç†æœ€çµ‚å…§å®¹ï¼š
===FINAL_OUTPUT_START===
@@VIDEO_SCRIPT@@
[å®Œæ•´çš„å½±ç‰‡è…³æœ¬å…§å®¹]
@@VIDEO_SCRIPT_END@@
@@SOCIAL_MEDIA@@
[å®Œæ•´çš„ç¤¾ç¾¤åª’é«”å…§å®¹]
@@SOCIAL_MEDIA_END@@
===FINAL_OUTPUT_END==="

å¦‚æœç™¼ç¾ä»»ä½•å•é¡Œï¼Œè«‹æ˜ç¢ºæŒ‡å‡ºéœ€è¦æ”¹é€²çš„åœ°æ–¹ï¼Œä¸¦è¦æ±‚ç›¸é—œ Agent ä¿®æ­£ã€‚
"""
from tools.web_search import WebSearch
from tools.web_scraper_tools import fetch_webpage, execute_python_code, validate_scraped_data, generate_scraping_template

logging.basicConfig(level=logging.INFO)


def get_llm_config() -> Dict:
    """
    Constructs the language model configuration for AutoGen agents.

    Returns:
        Dict: A dictionary containing the LLM configuration.
    """
    return {
        "model": settings.OPENAI_MODEL_NAME,
        "api_key": settings.OPENAI_API_KEY,
        "base_url": settings.OPENAI_API_BASE,
        "extra_body": {
                "chat_template_kwargs": {"enable_thinking": False},
                },
    }


def run_workflow(topic: str, progress_callback=None, selected_topic_data: dict = None) -> Dict[str, str]:
    """
    Orchestrates the collaboration between AI agents to generate content for a given topic.

    Args:
        topic (str): The topic to research and write about.
        progress_callback (callable, optional): Function to call with progress updates.
        selected_topic_data (dict, optional): Complete topic data including news URLs from trends.

    Returns:
        Dict[str, str]: A dictionary containing the generated 'video_script' and 'social_media' content.
    """
    llm_config = get_llm_config()
    
    # åˆå§‹åŒ–é€²åº¦è¿½è¹¤
    if progress_callback:
        progress_callback("ğŸš€ åˆå§‹åŒ– AI Agents...")
    
    # Initialize the web search tool
    web_search_tool = WebSearch()

    # Define the UserProxyAgent that will execute function calls
    user_proxy = UserProxyAgent(
        name="user_proxy",
        is_termination_msg=lambda x: (
            x.get("content", "") and 
            ("===FINAL_OUTPUT_END===" in x.get("content", "") or 
             "WORKFLOW_COMPLETE" in x.get("content", "") or 
             "FINAL_CONTENT" in x.get("content", ""))
        ),
        human_input_mode="NEVER",
        max_consecutive_auto_reply=20,
        code_execution_config=False,
    )

    # Define the Researcher Agent
    researcher = AssistantAgent(
        name="Trend_Researcher",
        system_message=RESEARCHER_PROMPT.format(topic=topic),
        llm_config=llm_config,
    )
    
    # Register the search function for the researcher
    def search_for_topic(query: str) -> str:
        """Function to be called by the researcher agent to search the web."""
        # æå–è¶¨å‹¢æ–°è URL
        trend_urls = []
        if selected_topic_data and selected_topic_data.get('news_items'):
            trend_urls = [news.get('url') for news in selected_topic_data['news_items'] if news.get('url')]
            if progress_callback and trend_urls:
                progress_callback(f"ğŸ—ï¸ ç™¼ç¾ {len(trend_urls)} å€‹è¶¨å‹¢ç›¸é—œæ–°èï¼Œæ­£åœ¨çˆ¬å–æœ€æ–°å…§å®¹...")
            logging.info(f"å¾è¶¨å‹¢æ•¸æ“šæå–åˆ° {len(trend_urls)} å€‹ç›¸é—œæ–°èURL")
        
        results = web_search_tool.search(query, trend_urls=trend_urls)
        
        if progress_callback:
            total_results = len(results) if results else 0
            trend_count = sum(1 for r in results if r.get('source') == 'Trending News')
            tavily_count = sum(1 for r in results if r.get('source') == 'Tavily')
            progress_callback(f"ğŸ“š ç ”ç©¶å®Œæˆï¼šç²å– {total_results} å€‹è³‡æ–™ä¾†æº (æœ€æ–°æ–°è: {trend_count}, Tavily: {tavily_count})")
        
        return str(results)

    # ç‚º Web Scraper å®šç¾©å·¥å…·å‡½æ•¸
    def fetch_webpage_content(url: str) -> str:
        """ç”± Web Scraper Agent èª¿ç”¨ä¾†ç²å–ç¶²é å…§å®¹"""
        if progress_callback:
            progress_callback(f"ğŸŒ æ­£åœ¨ç²å–ç¶²é : {url}")
        return fetch_webpage(url)
    
    def execute_scraping_code(code: str) -> str:
        """ç”± Web Scraper Agent èª¿ç”¨ä¾†åŸ·è¡Œçˆ¬èŸ²ç¨‹å¼ç¢¼"""
        if progress_callback:
            progress_callback("ğŸ”§ åŸ·è¡Œè‡ªå®šç¾©çˆ¬èŸ²ç¨‹å¼ç¢¼...")
        result = execute_python_code(code)
        return str(result)
    
    user_proxy.register_function(
        function_map={
            "search_for_topic": search_for_topic,
            "fetch_webpage_content": fetch_webpage_content,
            "execute_scraping_code": execute_scraping_code
        }
    )
    
    researcher.register_function(
        function_map={
            "search_for_topic": search_for_topic
        }
    )


    # Define the Script Writer Agent
    script_writer = AssistantAgent(
        name="Script_Writer",
        system_message=SCRIPT_WRITER_PROMPT,
        llm_config=llm_config,
    )

    # Define the Social Media Writer Agent
    social_writer = AssistantAgent(
        name="Social_Media_Writer",
        system_message=SOCIAL_WRITER_PROMPT,
        llm_config=llm_config,
    )

    # Define the Web Scraper Agent
    web_scraper = AssistantAgent(
        name="Web_Scraper",
        system_message=WEB_SCRAPER_PROMPT,
        llm_config=llm_config,
    )
    
    # Register web scraping functions for the Web Scraper Agent
    web_scraper.register_function(
        function_map={
            "fetch_webpage_content": fetch_webpage_content,
            # "execute_scraping_code": execute_scraping_code
        }
    )

    # Define the Workflow Coordinator Agent
    coordinator = AssistantAgent(
        name="Workflow_Coordinator",
        system_message=COORDINATOR_PROMPT,
        llm_config=llm_config,
    )
    
    if progress_callback:
        progress_callback("ğŸ¤– AI Agents çµ„å»ºå®Œæˆï¼Œé–‹å§‹å”ä½œ...")

    # Create the GroupChat and Manager
    agents: List[Agent] = [user_proxy, researcher, web_scraper, script_writer, social_writer, coordinator]
    groupchat = GroupChat(agents=agents, messages=[], max_round=20)
    manager = GroupChatManager(groupchat=groupchat, llm_config=llm_config)

    # Initial message to kick off the process
    initial_message = f"""
    ä¸»é¡Œé¸å®šï¼š"{topic}"

    å·¥ä½œæµç¨‹è¨ˆåŠƒï¼š
    1.  **Trend_Researcher**: ä½¿ç”¨ `search_for_topic` å·¥å…·ç ”ç©¶ä¸»é¡Œ '{topic}'ã€‚å°‡ç™¼ç¾æ•´ç†æˆçµæ§‹åŒ– JSON æ ¼å¼ï¼ŒåŒ…å«é—œéµé»ã€çµ±è¨ˆè³‡æ–™å’Œä¾†æºã€‚
    2.  **Web_Scraper**: é‡å°ç ”ç©¶éç¨‹ä¸­ç™¼ç¾çš„é‡è¦ URLï¼Œä½¿ç”¨ `fetch_webpage_content` ç²å–ç¶²é ï¼Œåˆ†æçµæ§‹å¾Œç”¨ `execute_scraping_code` åŸ·è¡Œå®¢è£½åŒ–çˆ¬èŸ²ç¨‹å¼ç¢¼ï¼Œæä¾›æ›´è©³ç´°çš„çµæ§‹åŒ–è³‡æ–™ã€‚
    3.  **Script_Writer**: ç ”ç©¶å’Œçˆ¬èŸ²å®Œæˆå¾Œï¼Œä½¿ç”¨æ‰€æœ‰çµæ§‹åŒ–è³‡æ–™æ’°å¯« 60 ç§’å½±ç‰‡è…³æœ¬ã€‚
    4.  **Social_Media_Writer**: æ ¹æ“šç ”ç©¶ã€çˆ¬èŸ²è³‡æ–™å’Œå½±ç‰‡è…³æœ¬ï¼Œç‚º Instagram/Facebookã€X/Twitter å’Œ LinkedIn å‰µä½œç¤¾ç¾¤åª’é«”æ–‡æ¡ˆã€‚
    5.  **Workflow_Coordinator**: ç›£æ§æ‰€æœ‰ä»»å‹™é€²åº¦ï¼Œç¢ºä¿å“è³ªï¼Œç•¶æ‰€æœ‰å·¥ä½œå®Œæˆæ™‚å®£å¸ƒ "WORKFLOW_COMPLETE" ä¸¦è¦æ±‚ user_proxy æ ¼å¼åŒ–è¼¸å‡ºã€‚
    5.  **user_proxy**: ç•¶å”èª¿è€…ç¢ºèªå·¥ä½œå®Œæˆå¾Œï¼Œå¿…é ˆæŒ‰ç…§ä»¥ä¸‹ç²¾ç¢ºæ ¼å¼æ•´ç†æœ€çµ‚è¼¸å‡ºï¼š
        
        ===FINAL_OUTPUT_START===
        @@VIDEO_SCRIPT@@
        [å®Œæ•´çš„60ç§’å½±ç‰‡è…³æœ¬å…§å®¹]
        @@VIDEO_SCRIPT_END@@
        @@SOCIAL_MEDIA@@
        [å®Œæ•´çš„ç¤¾ç¾¤åª’é«”æ–‡æ¡ˆå…§å®¹]
        @@SOCIAL_MEDIA_END@@
        ===FINAL_OUTPUT_END===

    è«‹é–‹å§‹åŸ·è¡Œï¼
    """
    
    if progress_callback:
        progress_callback("ğŸ” Trend_Researcher æ­£åœ¨ç ”ç©¶ä¸»é¡Œ...")

    # Start the chat
    if progress_callback:
        progress_callback("ğŸ”„ é–‹å§‹ AI Agents å”ä½œå°è©±...")
    
    user_proxy.initiate_chat(
        manager,
        message=initial_message,
    )
    
    # æ ¹æ“šå°è©±æ­·å²æ›´æ–°é€²åº¦
    if progress_callback:
        messages = groupchat.messages
        for msg in messages[-10:]:  # æª¢æŸ¥æœ€å¾Œ10æ¢æ¶ˆæ¯
            sender = msg.get("name", "")
            content = msg.get("content", "")
            
            if sender == "Web_Scraper" and len(content) > 50:
                progress_callback("ğŸ•·ï¸ Web_Scraper æ­£åœ¨åˆ†æå’Œçˆ¬å–ç¶²é ...")
            elif sender == "Script_Writer" and len(content) > 50:
                progress_callback("ğŸ¬ Script_Writer æ­£åœ¨å‰µä½œå½±ç‰‡è…³æœ¬...")
            elif sender == "Social_Media_Writer" and len(content) > 50:
                progress_callback("ğŸ“± Social_Media_Writer æ­£åœ¨æ’°å¯«ç¤¾ç¾¤æ–‡æ¡ˆ...")
            elif sender == "Workflow_Coordinator" and "WORKFLOW_COMPLETE" in content:
                progress_callback("âœ… Workflow_Coordinator ç¢ºèªæ‰€æœ‰ä»»å‹™å®Œæˆ...")
        
        progress_callback("ğŸ“‹ æ­£åœ¨æå–ç”Ÿæˆçš„å…§å®¹...")

    # Extract the final content with improved parsing logic
    def extract_content_from_messages(messages: List[Dict]) -> Dict[str, str]:
        """å¾æ¶ˆæ¯æ­·å²ä¸­æ™ºèƒ½æå–å…§å®¹"""
        video_script = None
        social_media = None
        
        # æª¢æŸ¥æ‰€æœ‰æ¶ˆæ¯ï¼Œå°‹æ‰¾å…§å®¹
        for msg in reversed(messages):  # å¾æœ€æ–°æ¶ˆæ¯é–‹å§‹æª¢æŸ¥
            content = msg.get("content", "")
            sender = msg.get("name", "")
            
            # å„ªå…ˆä½¿ç”¨æ–°æ ¼å¼
            if "===FINAL_OUTPUT_START===" in content:
                try:
                    if "@@VIDEO_SCRIPT@@" in content and "@@SOCIAL_MEDIA@@" in content:
                        video_part = content.split("@@VIDEO_SCRIPT@@")[1].split("@@VIDEO_SCRIPT_END@@")[0].strip()
                        social_part = content.split("@@SOCIAL_MEDIA@@")[1].split("@@SOCIAL_MEDIA_END@@")[0].strip()
                        return {"video_script": video_part, "social_media": social_part}
                except (IndexError, AttributeError):
                    continue
            
            # å˜—è©¦èˆŠæ ¼å¼
            if "---VIDEO_SCRIPT_START---" in content:
                try:
                    video_part = content.split("---VIDEO_SCRIPT_START---")[1].split("---VIDEO_SCRIPT_END---")[0].strip()
                    if len(video_part) > 50:
                        video_script = video_part
                except (IndexError, AttributeError):
                    continue
                    
            if "---SOCIAL_MEDIA_START---" in content:
                try:
                    social_part = content.split("---SOCIAL_MEDIA_START---")[1].split("---SOCIAL_MEDIA_END---")[0].strip()
                    if len(social_part) > 30:
                        social_media = social_part
                except (IndexError, AttributeError):
                    continue
            
            # æ™ºèƒ½è­˜åˆ¥å…§å®¹é¡å‹
            if sender == "Script_Writer" and len(content) > 100:
                if not video_script and any(keyword in content.lower() for keyword in 
                    ["è…³æœ¬", "script", "å½±ç‰‡", "video", "æ—ç™½", "é–‹å ´"]):
                    video_script = content
                    
            elif sender == "Social_Media_Writer" and len(content) > 50:
                if not social_media and any(keyword in content.lower() for keyword in 
                    ["ç¤¾ç¾¤", "social", "instagram", "facebook", "twitter", "linkedin", "è²¼æ–‡"]):
                    social_media = content
        
        return {
            "video_script": video_script or "ç„¡æ³•æå–å½±ç‰‡è…³æœ¬å…§å®¹",
            "social_media": social_media or "ç„¡æ³•æå–ç¤¾ç¾¤åª’é«”å…§å®¹"
        }
    
    final_message = user_proxy.last_message()["content"]
    logging.info(f"Final message received: {final_message[:200]}...")
    
    # å˜—è©¦å¤šç¨®æå–æ–¹æ³•
    content_extracted = False
    video_script = None
    social_media = None
    
    try:
        # æ–¹æ³•1: å˜—è©¦æ–°æ ¼å¼
        if "===FINAL_OUTPUT_START===" in final_message:
            if "@@VIDEO_SCRIPT@@" in final_message and "@@SOCIAL_MEDIA@@" in final_message:
                video_script = final_message.split("@@VIDEO_SCRIPT@@")[1].split("@@VIDEO_SCRIPT_END@@")[0].strip()
                social_media = final_message.split("@@SOCIAL_MEDIA@@")[1].split("@@SOCIAL_MEDIA_END@@")[0].strip()
                content_extracted = True
                logging.info("âœ… @@æ ¼å¼æˆåŠŸæå–å…§å®¹")
        
        # æ–¹æ³•2: å˜—è©¦èˆŠæ ¼å¼
        if not content_extracted and "---VIDEO_SCRIPT_START---" in final_message:
            video_script = final_message.split("---VIDEO_SCRIPT_START---")[1].split("---VIDEO_SCRIPT_END---")[0].strip()
            social_media = final_message.split("---SOCIAL_MEDIA_START---")[1].split("---SOCIAL_MEDIA_END---")[0].strip()
            content_extracted = True
            logging.info("âœ… ä½¿ç”¨èˆŠæ ¼å¼æˆåŠŸæå–å…§å®¹")
        
        # æ–¹æ³•3: å¾å°è©±æ­·å²æ™ºèƒ½æå–
        if not content_extracted:
            logging.warning("ğŸ” æ¨™æº–æ ¼å¼æœªæ‰¾åˆ°ï¼Œé–‹å§‹æ™ºèƒ½å…§å®¹æå–...")
            extracted_content = extract_content_from_messages(groupchat.messages)
            video_script = extracted_content["video_script"]
            social_media = extracted_content["social_media"]
            logging.info("ğŸ“‹ æ™ºèƒ½æå–å®Œæˆ")
            
    except Exception as e:
        logging.error(f"âŒ å…§å®¹æå–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        # æœ€å¾Œå˜—è©¦ï¼šå¾å°è©±æ­·å²æå–
        extracted_content = extract_content_from_messages(groupchat.messages)
        video_script = extracted_content["video_script"]
        social_media = extracted_content["social_media"]
    
    # ç¢ºä¿è¿”å›æœ‰æ•ˆå…§å®¹
    if not video_script or len(video_script.strip()) < 20:
        video_script = "å…§å®¹ç”Ÿæˆéç¨‹ä¸­å‡ºç¾å•é¡Œï¼Œè«‹é‡æ–°å˜—è©¦ã€‚"
    if not social_media or len(social_media.strip()) < 10:
        social_media = "å…§å®¹ç”Ÿæˆéç¨‹ä¸­å‡ºç¾å•é¡Œï¼Œè«‹é‡æ–°å˜—è©¦ã€‚"
        
    logging.info(f"ğŸ“Š æœ€çµ‚æå–çµæœ - å½±ç‰‡è…³æœ¬é•·åº¦: {len(video_script)}, ç¤¾ç¾¤å…§å®¹é•·åº¦: {len(social_media)}")
    
    # è¨˜éŒ„è©³ç´°çš„å…§å®¹ç‰‡æ®µä¾›èª¿è©¦
    logging.info(f"ğŸ¬ å½±ç‰‡è…³æœ¬å‰100å­—ç¬¦: {video_script[:100]}...")
    logging.info(f"ğŸ“± ç¤¾ç¾¤å…§å®¹å‰100å­—ç¬¦: {social_media[:100]}...")

    return {
        "video_script": video_script,
        "social_media": social_media,
    }


if __name__ == '__main__':
    # Example usage:
    # Ensure .env file has the necessary API keys
    test_topic = "NVIDIA GTC 2024"
    content = run_workflow(test_topic)
    print("--- Generated Video Script ---")
    print(content["video_script"])
    print("\n--- Generated Social Media Content ---")
    print(content["social_media"])
